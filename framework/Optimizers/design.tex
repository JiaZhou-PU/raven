\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{graphicx}%
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{cite}

%\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{physics}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\expv}[1]{\ensuremath{\mathbb{E}[ #1]}}
\newcommand{\xs}[2]{\ensuremath{\Sigma_{#1}^{(#2)}}}
\newcommand{\intO}{\ensuremath{\int\limits_{4\pi}}}
\newcommand{\intz}{\ensuremath{\int\limits_0^1}}
\newcommand{\intf}{\ensuremath{\int\limits_{-\infty}^\infty}}
\newcommand{\intzf}{\ensuremath{\int\limits_{0}^\infty}}
\newcommand{\LargerCdot}{\raisebox{-0.25ex}{\scalebox{1.2}{$\cdot$}}}

\textwidth6.6in
\textheight9in

% \graphicspath{{../graphics/}}

\setlength{\topmargin}{0.3in} \addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}

\setlength{\oddsidemargin}{0in}

\oddsidemargin  0.0in \evensidemargin 0.0in \parindent0em

%\pagestyle{fancy}\lhead{MATH 579 (UQ for PDEs)} \rhead{02/24/2014}
%\chead{Project Proposal} \lfoot{} \rfoot{\bf \thepage} \cfoot{}


\begin{document}

\title{Optimizer Design Use Cases and Requirements}

\author[]{RAVEN}
\affil[]{Nuclear Engineering Methods Development, Idaho National Laboratory}
%\date{}
\renewcommand\Authands{ and }
\maketitle
\newpage
\section{Introduction}
This being the documentation of design requirements for the Optimizer within the RAVEN framework.

\section{User Cases}
\subsection{Introduction}
User cases describe the way in which the Optimizer in RAVEN has been used, is being used, and is
planned to be used in the immediate and foreseeable future. This guides the generation of
requirements so that these user cases can be performed.

\subsection{Economic optimization of stochastic system}
In this use case, the model has several components which consume or produce resources, with
particular economic repurcussions for their actions. The use case is to optimize the sizing of the
compononents so that they can meet the needs of the externally-provided stochastic demand in the
most economic way possible.

In this scenario, there is a two-level optimization. In the outer level, we optimize the sizing of
the components to be as economic as possible while still meeting the demand. In the inner level, we
generate many synthetic demands, and for each determine the ideal dispatch of the system of
components to meet the demand as economically as possible. The average economic behavior of the
inner runs then indicates the economic viability of the set of component sizes.

\section{Requirements}
To organize the variety of optimization options, we propose a structure with a base Optimizer class,
containing all the methods and API definitions common to all optimizers.

Inheriting from the Optimizer are the following main categories of classes:
\begin{itemize}
  \item Interfaced, which use a callable library such as Pyomo, Gekko, or Scipy;
  \item Sampled, which use RAVEN's adaptive sampling system.
\end{itemize}

Interfaced optimizers are divided into the interfaces for the libraries they call.

Sampled optimizers can be further divided:
\begin{itemize}
  \item Gradient, which use gradients (or Hessians) to explore the domain for minima, e.g. FiniteDifference;
  \item Global, which use non-gradient techniques to explore the domain, e.g. Simulated Annealing.
\end{itemize}
Potential categories for additional Sampled optimizers may include Heuristic, Genetic Algorithms, etc.

\subsection{Optimizer Base Class}
The Optimizer inherits from the Sampler class.
\begin{itemize}
  \item The Optimizer class must implement the common API for RAVEN's use of the class as a Sampler
  Actor within the Step system. Most of this may be inherited from the Sampler base class.
  \item The Optimizer class must implement counters for multiple trajectories.
  \item The Optimizer class must implement methods to capture stochastic effects, e.g. expected
  value sampling.
\end{itemize}

\subsection{Interfaced Class}
The Interfaced Class inherits from the Optimizer and specializes for accessing optimization
algorithms in external libraries, such as Pyomo, Gekko, and Scipy.
\begin{itemize}
  \item The Interfaced class must implement the common API for interfacing with external libraries.
  This API must be sufficiently abstract as to accomodate varying interfaces.
  \item The Interfaced class must handle receiving requests from RAVEN for inputs (\texttt{generateInput})
  in the manner expected by the MultiRun Step. The class must return an object suitable to operation
  by the downstream members of the Step, such as the JobHandler and Runner.
\end{itemize}

\subsection{Sampled Class}
The Sampled Class inherits from the Optimizer and generalizes the optimization algorithms that make
use of RAVEN's iterative sampling scheduler.
\begin{itemize}
  \item The Sampled Class must provide the API for queuing required samples to provide to RAVEN.
  \item The Sampled Class must provide the API for labelling and retrieval of particular samples.
  \item The Sampled Class must provide the API for checking convergence on request.
\end{itemize}

\subsection{Gradient Class}
The Gradient class inherits from the Sampled class and generalizes the various gradient-based
sampling strategies. Notably, the features of gradient-based sampling can be divided into two
choices that are intercompatible: how to treat the iterative Step Size, and how to treat the
approximation of the Gradient at each juncture.
\begin{itemize}
  \item The Gradient class must provide the API for methods to locally estimate a Gradient.
  \item The Gradient class must provide the API for methods to modify the Step Size.
  \item The Gradient class must provide a method to store iterative measures such as gradient, evaluation, and
  step size histories.
\end{itemize}

\subsection{Global Class}
The Global Class inherits from the Sampled class and generalizes non-local, non-gradient-based
sampling strategies for optimization. Currently such algorithms are not within RAVEN, but space
should be made to include them at a future time.


%
%
%
%
\section{Features of Previous Optimizer}
These are the features of the optimizer in January 2020 prior to rework:
\begin{section}
  \item Variables
    \begin{itemize}
      \item Initial condition(s) specifiable or sampleable
      \item Upper and lower bound conditions specified (user option, required)
      \item Constants and dependent constants (Functions) handled as with Samplers
      \item Specification of the objective variable
    \end{itemize}
  \item Step sizing
    \begin{itemize}
      \item Specification of normalized step size (user option, defaulted)
      \item Specification of step growth/shrink via factor, based on dot-product of gradient (user option, defaulted)
      \item Step size reset on boundary/constraint violation
      \item Step size reduction as a function of number of iterations (minor, user option, defaulted)
    \end{itemize}
  \item Gradient Specification
    \begin{itemize}
      \item SPSA-ish
      \item Finite Difference, Central Difference
      \item Conjugate Gradient
      \item Neighbor points determined by Bernoulli, Hypersphere
      \item Distance for neighbor point evaluation (user option, defaulted)
    \end{itemize}
  \item Stochastic treatment
    \begin{itemize}
      \item Number of re-evaluations to average over (could be base Sampler feature)
    \end{itemize}
  \item Constraints
    \begin{itemize}
      \item Explicit (input-based) constraint handling via arbitrary Python function
      \item Path search along constraint by orthogonality search
      \item Step size resetting on constraint violation
      \item Limit constraint boundary search for possible trajectories
    \end{itemize}
  \item Multitrajectory
    \begin{itemize}
      \item Specify multiple starting locations (trajectories)
      \item Sampled from distributions as a Sampler
      \item Remove trajectories when they are sufficiently proximate (user setting, defaulted)
    \end{itemize}
  \item Convergence
    \begin{itemize}
      \item Limit by number of iterative steps
      \item Limit by number of model evaluations
      \item Recalculate gradient on rejection
      \item Convergence persistence (consecutive converges required)
      \item Converge on absolute, relative gradient magnitude
      \item Converge on absolute, relative change in objective variable
      \item Converge on minimum step size
    \end{itemize}
  \item Output
    \begin{itemize}
      \item Output of all evaluations to a data object
      \item Output of each iteration summary to a data object
      \item Option to only output the final solution
    \end{itemize}
  \item (deprecated) Multilevel Optimization
    \begin{itemize}
      \item Independent domain clustering and optimization iteration
      \item Preconditioning between level switching
    \end{itemize}
\end{section}
\end{document}